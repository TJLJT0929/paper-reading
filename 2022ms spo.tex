\subsection{引言}

在运筹学的许多现实世界分析应用中，机器学习与优化的结合被用于决策。通常，优化模型用于生成决策，而机器学习工具用于生成预测模型，以预测优化模型的关键未知参数。由于这两项任务固有的复杂性，分析实践中经常采用的一种通用方法是“先预测后优化”范式。

例如，考虑一个可能一天解决数次的车辆路径问题。首先，一个预先训练好的预测模型基于当前交通、天气、节假日、时间等，为道路网络所有边上的行程时间提供预测。然后，一个优化求解器使用预测的行程时间作为输入，提供近乎最优的路线。我们强调，现实世界分析问题的大多数求解系统都包含预测和优化的某些组成部分。除了少数有限选项外，机器学习工具并未有效考虑预测将如何在下游优化问题中使用。在本文中，我们提出了一个称为智能“先预测后优化”（SPO）的通用框架，用于训练预测模型，该框架有效利用了名义优化问题的结构——即其约束和目标。我们的 SPO 框架从根本上旨在生成以最小化决策误差（而非预测误差）为目标的预测模型。

我们 SPO 方法的一个关键优势在于它保持了顺序预测然后优化的决策范式。然而，在训练我们的预测模型时，明确使用了名义优化问题的结构。预测的质量不是基于预测误差（例如最小二乘损失或其他流行的损失函数）来衡量的。相反，在 SPO 框架中，预测的质量是通过决策误差来衡量的。也就是说，假设一个预测模型使用历史特征数据 \((x_{1},\ldots ,x_{n})\) 和相关的参数数据 \((c_{1},\dots,c_{n})\) 进行训练。令 \((\hat{c}_1,\dots,\hat{c}_n)\) 表示在训练模型下对参数的预测。例如，最小二乘（LS）损失使用平方范数 \(\| c_i - \hat{c}_i\| _2^2\) 来衡量误差，完全忽略了由预测引起的决策。相比之下，SPO 损失是由 \(\hat{c}_i\) 引起的决策的真实成本减去真实参数 \(c_{i}\) 下的最优成本。在车辆路径规划的背景下，SPO 损失衡量的是由于基于预测的（而非真实的）边成本参数求解路径问题而产生的额外行程时间。

在本文中，我们专注于预测上下文随机优化问题的未知参数，这些参数在线性目标函数中呈现——即任何线性、凸或整数优化问题的成本向量。我们 SPO 框架的核心是一个用于训练预测模型的新损失函数。由于 SPO 损失函数难以处理，大量工作围绕推导一个代理损失函数 \(\mathrm{SPO+}\) 展开，该函数是凸的，因此可以高效优化。为了证明代理 \(\mathrm{SPO+}\) 损失的有效性，我们证明了一个非常理想的统计一致性性质，并表明与标准的“先预测后优化”方法相比，它在经验上表现良好。本质上，我们证明了最小化与 \(\mathrm{SPO+}\) 损失相关的贝叶斯风险的函数是回归函数 \(\mathbb{E}[c|x]\)，该函数也最小化 SPO 损失的贝叶斯风险（在温和假设下）。有趣的是，在相同条件下，\(\mathbb{E}[c|x]\) 也最小化与 LS 损失相关的贝叶斯风险。因此，\(\mathrm{SPO+}\) 和 LS（或两者的任何凸组合）基本上处于“同等地位”——它们对于损失函数来说都是理论上有效（一致）且计算上易处理的选择。然而，当最终目标是解决下游优化任务时，\(\mathrm{SPO+}\) 损失是自然的选择，因为它针对优化问题量身定制，并且在实践中比 LS 表现得好得多。

根据经验，我们观察到，即使预测任务因模型设定错误而具有挑战性，SPO 框架仍能产生近乎最优的决策。我们注意到 SPO 框架的一个基本属性是要求预测直接“插入”到下游优化问题中。另一种替代程序可能会以某种方式改变决策过程，例如通过增加鲁棒性或考虑整个数据集（而不仅仅是预测）。我们 SPO 方法的一个强大优势是，即使朴素的预测问题具有挑战性，它也能具有良好的性能；参见第 3.1 节中的说明性示例。另一个优势是，与更复杂的替代程序相比，下游优化问题通常在计算上更易于处理，并且对从业者更具吸引力。另一方面，替代决策程序可能提供其他优势，例如通过引入偏差和/或鲁棒性来提高泛化性能。然而，在存在上下文数据的情况下设计此类程序更具挑战性，将它们与 SPO 方法结合将是未来研究值得的方向。总体而言，我们相信我们的 SPO 框架为设计可在现实世界优化设置中利用的运营驱动的机器学习（ML）工具提供了清晰的基础。

我们的贡献可总结如下：

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.}
	\item
	我们首先正式定义了一个新的损失函数，称之为 SPO 损失，它用于预测具有线性、凸或整数约束的名义优化问题的成本向量时的误差。该损失对应于由于实施由预测成本向量引起的可能不正确的决策而导致的（相对于真实/历史成本向量的）次优性差距。不幸的是，SPO 损失函数在预测中可能是非凸且不连续的，这意味着在 SPO 损失下训练 ML 模型可能具有挑战性。\\
	\item
	鉴于 SPO 损失函数的难处理性，我们开发了一个代理损失函数，称之为 \(\mathrm{SPO + }\) 损失。这个代理损失函数是通过使用一系列步骤推导出来的，这些步骤的动机来自于对偶理论（命题 2）、数据缩放近似和一阶近似。所得的 \(\mathrm{SPO + }\) 损失函数在预测中是凸的（命题 3），这使我们能够设计一个基于随机梯度下降的算法来最小化 \(\mathrm{SPO + }\) 损失（命题 8）。此外，当训练线性回归模型来预测线性规划的目标系数时，只需要解决一个线性优化问题即可最小化 \(\mathrm{SPO + }\) 损失（命题 7）。\\
	\item
	在我们 SPO 框架的一个非常简单和特殊的实例下，我们证明了与经典机器学习的基本联系。即，在此实例下，SPO 损失恰好是 0-1 分类损失（命题 1），而 \(\mathrm{SPO + }\) 损失恰好是合页损失（命题 4）。合页损失是流行的支持向量机（SVM）方法的基础，并且是近似最小化 0-1 损失的代理损失，因此，我们的框架将这一概念推广到具有约束的非常广泛的优化问题族。
	\item
	我们证明了 SPO+ 损失函数的一个关键一致性结果（定理 1、命题 5 和命题 6），这进一步推动了其使用。即，在完全分布知识下，如果满足两个温和条件：成本向量（给定特征）的分布是连续的且关于其均值对称，那么最小化 SPO+ 损失函数实际上等同于最小化 SPO 损失。例如，标准高斯噪声近似满足这些假设。这种一致性性质在统计学和机器学习文献中被广泛认为是任何代理损失函数的基本属性。例如，著名的合页损失和逻辑损失函数与 0-1 分类损失是一致的。
	\item
	最后，我们通过最短路径和投资组合优化问题的数值实验验证了我们的框架。我们针对标准的“先预测后优化”方法测试了我们的 SPO 框架，并评估了其在 SPO 损失下的样本外性能。通常，随着模型设定错误程度的增加，我们 SPO 框架的价值也会增加。这正是因为 SPO 框架做出了“更好”的错误预测，本质上“欺骗”优化问题找到近乎最优的解。值得注意的是，即使真实情况是高度非线性的，使用 \(\mathrm{SPO + }\) 训练的线性模型甚至优于最先进的随机森林算法。
\end{enumerate}

\subsubsection{应用}

需要根据上下文（特征）数据预测优化问题的输入参数（成本向量）的场景非常多。现在让我们重点介绍 SPO 框架的少数几个（可能有很多）应用领域。

1.1.1. 车辆路径规划。在众多应用中，需要在做出路径决策之前预测图中每条边的成本。边的成本通常对应于车辆穿越相应边所需的预期时间长度。为清晰起见，让我们关注一个重要例子——最短路径问题。在最短路径问题中，给定一个带权有向图，以及起点节点和终点节点，目标是找到从起点到终点的边序列，使得总成本尽可能小。一个众所周知的事实是，最短路径问题可以表述为线性优化问题，但也有其他专门的算法，例如著名的 Dijkstra 算法（参见，例如，Ahuja 等人 1993）。用于预测边成本的数据可能包括长度、速度限制、天气、季节、日期以及来自移动应用程序（如 Google Maps 和 Waze）的实时数据。

仅仅最小化预测误差可能不够或不合适，因为高估或低估在整个网络中具有截然不同的影响。SPO 框架将确保预测的权重能够产生最短路径，并自然强调对关键边成本的估计。参见第 3.1 节以获取深入示例。

1.1.2. 库存管理。在库存计划问题中，例如经济批量问题（Wagner 和 Whitin 1958）或联合补货问题（Levi 等人 2006），需求是优化模型的关键输入。在实际环境中，需求是非常不稳定的，并且可能依赖于历史和上下文数据，例如天气、季节性和竞争对手销售。何时订购库存的决策由线性或整数优化模型捕获，具体取决于问题的复杂性。在常见的表述下（参见 Levi 等人 2006 和 Cheung 等人 2016），需求线性出现在目标函数中，这对 SPO 框架很方便。目标是设计一个预测模型，将特征数据映射到需求预测，从而产生良好的库存计划。

1.1.3. 投资组合优化。在金融服务应用中，潜在投资的回报需要以某种方式从数据中估计，并且可能依赖于许多特征，这些特征通常包括历史回报、新闻、经济因素、社交媒体等。在投资组合优化中，目标是在满足投资组合总风险或方差约束的条件下，找到具有最高回报的投资组合。尽管回报通常高度依赖于辅助特征信息，但方差通常稳定得多，并且不那么难以预测或对预测敏感。我们的 SPO 框架将产生导致满足期望风险水平的高性能投资的预测。最小二乘损失方法更侧重于估计价值较高的投资，即使相应的风险可能不理想。相比之下，SPO 框架在训练预测模型时直接考虑了每项投资的风险。

\subsubsection{相关文献}

也许最相关的工作是 Kao 等人（2009）的研究，他们也直接寻求训练一个机器学习模型，以最小化相对于名义优化问题的损失。在他们的框架中，名义问题是一个无约束的二次优化问题，其中未知参数出现在目标的线性部分。他们的工作没有扩展到名义优化问题存在约束的情况，而我们的框架可以。Donti 等人（2017）提出了一种启发式方法，以解决比 Kao 等人（2009）更一般的设置，并且也专注于二次优化的情况。这些工作也绕过了名义问题解非唯一性的问题（因为它们的问题是强凸的），而在我们的设置中必须解决这个问题以避免退化的预测模型。

在 Ban 和 Rudin（2019）中，训练 ML 模型以直接从数据预测报童问题的最优解。展示了该方法的可处理性和统计性质，以及其在实践中的有效性。然而，当存在约束时，尚不清楚如何使用这种方法，因为可能会出现可行性问题。

Bertsimas 和 Kallus（2020）中的通用方法考虑了使用 ML 模型准确估计未知优化目标的问题，其中预测可以描述为训练样本的加权组合——例如，最近邻和决策树。在他们的方法中，他们通过将 ML 模型生成的相同权重应用于这些样本的相应目标函数来估计实例的目标。仅当目标函数在未知参数中是非线性时，这种方法才与标准的“先预测后优化”不同。请注意，第 1.1 节中提到的所有应用的未知参数都线性出现在目标中。此外，与 SPO 框架相比，ML 模型的训练不依赖于名义优化问题的结构。

Tulabandhula 和 Rudin（2013）中的方法依赖于最小化一个损失函数，该函数将预测误差与模型在未标记数据集上的运营成本相结合。然而，运营成本是针对预测参数，而不是真实参数。Gupta 和 Rusmevichientong（2017）考虑了在没有特征/上下文的情况下结合估计与优化。我们还注意到，我们的 SPO 损失虽然在数学上不同，但在精神上类似于 Lim 等人（2012）在特定投资组合优化背景下（使用历史回报数据且无特征）引入的相对遗憾概念。其他从数据中寻找近似最优解的方法包括运营统计（Liyanage 和 Shanthikumar 2005, Chu 等人 2008）、样本平均近似（Kleywegt 等人 2002, Schutz 等人 2009, Bertsimas 等人 2018b）和鲁棒优化（Bertsimas 和 Thiele 2006, Wang 等人 2016, Bertsimas 等人 2018a）。在基于样本的子模优化方面也取得了一些近期进展（Balkanski 等人 2016, 2017）。这些方法通常没有明确的使用特征数据的方式，也没有直接考虑如何训练机器学习模型来预测优化参数。

另一个相关的工作流是数据驱动的逆优化，其中观察到优化问题的可行解或最优解，并且必须学习目标函数（Keshavarz 等人 2011, Chan 等人 2014, Bertsimas 等人 2015, Aswani 等人 2018, Esfahani 等人 2018）。在这些问题中，通常存在单个未知目标，并且不提供目标的先前样本。我们还注意到，在优化问题的背景下，最近出现了一些关于正则化（Ban 等人 2018）和模型选择（Besbes 等人 2010, Den Boer 和 Sierag 2020, Sen 和 Deng 2017）的方法。

最后，我们注意到我们的框架与结构化预测的一般设置相关（参见，例如，Taskar 等人 2005, Tsochantaridis 等人 2005, Nowozin 等人 2011, Osokin 等人 2017 及其参考文献）。受计算机视觉和自然语言处理问题的推动，结构化预测是多类分类的一个版本，涉及从特征数据预测结构化对象，例如序列或图。SPO+ 损失在精神上类似于结构化 SVM（SSVM），并且确实是 SPO 损失的一个凸上界，类似于 SSVM。然而，我们的方法与 SSVM 方法存在根本区别。在 SSVM 方法中，结构化对象是直接从特征 \(x\) 预测决策 \(w\)（Taskar 等人 2005）。在我们的设置中，我们可以访问关于 \(c\) 的历史数据，这比决策的观察更丰富，因为成本向量自然诱导出最优决策。在我们框架的一个特殊情况下，我们证明 SPO 损失等价于 \(0/1\) 损失，而 SPO+ 损失等价于合页损失。因此，我们的框架可以看作是 SSVM 的一种推广。最后，我们指出，我们推导代理 SPO+ 损失依赖于使用对偶理论的全新思想，这有助于解释其强大的经验性能。


\subsection{“先预测后优化”框架}\label{predict-then-optimize-framework}

我们现在描述“先预测后优化”（PO）框架，该框架是许多优化在实际应用中的核心。具体来说，我们假设存在一个感兴趣的名义优化问题，其具有线性目标，其中决策变量 \(w \in \mathbb{R}^d\) 和可行域 \(S \subseteq \mathbb{R}^d\) 是明确定义且确知的。然而，目标函数的成本向量 \(c \in \mathbb{R}^d\) 在必须做出决策时不可用；相反，一个相关的特征向量 \(x \in \mathbb{R}^p\) 是可用的。令 \(\mathcal{D}_x\) 为给定 \(x\) 时 \(c\) 的条件分布。决策者的目标是针对任何以 \(x\) 为特征的新实例，解决上下文随机优化问题

\[
\min  _ {w \in S} \mathbb {E} _ {c \sim \mathcal {D} _ {x}} \left[ c ^ {\top} w | x \right] = \min  _ {w \in S} \mathbb {E} _ {c \sim \mathcal {D} _ {x}} [ c | x ] ^ {\top} w. \tag {1}
\]

“先预测后优化”框架依赖于使用对 \(\mathbb{E}_{c\sim \mathcal{D}_x}[c|x]\) 的预测（我们记为 \(\hat{c}\)），并基于 \(\hat{c}\) 求解确定性版本的优化问题，即 \(\min_{w\in S}\hat{c}^{\top}w\)。本文我们的主要兴趣在于为“先预测后优化”框架定义合适的损失函数，检验它们的性质，并开发使用这些损失函数训练预测模型的算法。

我们现在正式列出我们框架的关键组成部分：

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.}
	\item
	名义（下游）优化问题，其形式如下：
\end{enumerate}

\[
\begin{array}{l} P (c): \quad z ^ {*} (c) := \min  _ {w} c ^ {T} w \\ \mathrm {s . t .} w \in S, \tag {2} \\ \end{array}
\]

其中 \(w \in \mathbb{R}^d\) 是决策变量，\(c \in \mathbb{R}^d\) 是描述线性目标函数的问题数据，\(S \subseteq \mathbb{R}^d\) 是一个非空、紧（即闭且有界）的凸集，代表可行域。因为我们这里关注的是线性优化问题，所以假设 \(S\) 是凸和闭的不会失去一般性。实际上，如果 (2) 中的 \(S\) 可能是非凸或非闭的，那么用其闭凸包替换 \(S\) 不会改变最优值 \(z^{*}(c)\)（Jaggi 2011 中的引理 8）。因此，线性优化问题的这种基本等价性意味着我们的方法可以应用于组合和混合整数优化问题，我们将在第 3.2 节进一步阐述。由于假设 \(S\) 是固定且确知的，每个问题实例都可以由相应的成本向量描述，因此 (2) 中依赖于 \(c\)。在解决 \(c\) 未知的特定实例时，将使用对 \(c\) 的预测来代替。我们假设可以访问一个实际高效的优化预言机 \(w^{*}(c)\)，它对于任何输入成本向量返回 \(P(c)\) 的一个解。例如，如果 (2) 对应于一个线性、锥或混合整数优化问题，那么一个商业优化求解器或专门算法就足以充当 \(w^{*}(c)\)。

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.}
	\setcounter{enumi}{1}
	\item
	形式为 \((x_{1},c_{1}),(x_{2},c_{2}),\ldots ,(x_{n},c_{n})\) 的训练数据，其中 \(x_{i}\in \mathcal{X}\) 是代表与 \(c_{i}\) 相关的上下文信息的特征向量。\\
	\item
	一个成本向量预测模型的假设类 \(\mathcal{H}\)，即 \(f: \mathcal{X} \to \mathbb{R}^d\)，其中 \(\hat{c} \coloneqq f(x)\) 被解释为与特征向量 \(x\) 相关的预测成本向量。\\
	\item
	一个损失函数 \(\ell(\cdot, \cdot): \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}_+\)，其中 \(\ell(\hat{c}, c)\) 量化了当实现（真实）成本向量实际上是 \(c\) 时，做出预测 \(\hat{c}\) 的误差。
\end{enumerate}

给定损失函数 \(\ell (\cdot ,\cdot)\) 和训练数据 \((x_{1},c_{1}),\ldots ,(x_{n},c_{n})\)，经验风险最小化（ERM）原则指出，我们应该通过求解优化问题来确定一个预测模型 \(f^{*}\in \mathcal{H}\)

\[
\min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \ell \left(f \left(x _ {i}\right), c _ {i}\right). \tag {3}
\]

在提供了预测模型 \(f^{*}\) 并给定一个特征向量 \(x\) 的情况下，“先预测后优化”决策规则是选择相对于预测成本向量的最优解——即 \(w^{*}(f^{*}(x))\)。在线附录 A 中的例 1 将我们的框架置于网络优化问题的背景下进行了说明。

在“先预测后优化”框架的标准应用中，如例 1 所示，所使用的损失函数完全独立于名义优化问题。换句话说，优化问题 \(P(\cdot)\) 的基本结构并未纳入损失函数，因此也未纳入预测模型的训练中。例如，当 \(\ell(\hat{c}, c) = \frac{1}{2} \| \hat{c} - c \|_2^2\) 时，这对应于最小二乘损失函数。此外，如果 \(\mathcal{H}\) 是一组线性预测器，那么 (3) 简化为一个标准的最小二乘线性回归问题。相比之下，我们在第 3 节的重点是构建通过利用问题结构来衡量预测成本向量时决策误差的损失函数。

\subsubsection{有用的符号表示}\label{useful-notation}

令 \(p\) 为特征向量的维度，\(d\) 为决策向量的维度，\(n\) 为训练样本的数量。令 \(W^{*}(c) \coloneqq \arg \min_{w \in S} \{c^{T} w\}\) 表示 \(P(\cdot)\) 的最优解集，并令 \(w^{*}(\cdot): \mathbb{R}^{d} \to S\) 表示一个特定的用于求解 \(P(\cdot)\) 的预言机。即，\(w^{*}(\cdot)\) 是一个固定的确定性映射，满足 \(w^{*}(c) \in W^{*}(c)\)。注意，没有对映射 \(w^{*}(\cdot)\) 做任何特殊假设；因此，\(w^{*}(c)\) 可被视为 \(W^{*}(c)\) 的任意一个元素。令 \(\xi_{s}(\cdot): \mathbb{R}^{d} \to \mathbb{R}\) 表示 \(S\) 的支撑函数，其定义为 \(\mathrm{supp}(c) \coloneqq \max_{w \in S} \{c^{T} w\}\)。由于 \(S\) 是紧的，\(\xi_{s}(\cdot)\) 处处有限，定义中的最大值对于每个 \(c \in \mathbb{R}^{d}\) 都可达到，并且注意对于所有 \(c \in \mathbb{R}^{d}\)，有 \(\mathrm{supp}(c) = -z^{*}(-c) = c^{T} w^{*}(-c)\)。还要回顾一下，\(\mathrm{supp}(\cdot)\) 是一个凸函数。对于给定的凸函数 \(h(\cdot): \mathbb{R}^{d} \to \mathbb{R}\)，回顾一下，如果对于所有 \(c' \in \mathbb{R}^{d}\) 满足 \(h(c') \geq h(c) + g^{T}(c' - c)\)，则 \(g \in \mathbb{R}^{d}\) 是 \(h(\cdot)\) 在 \(c \in \mathbb{R}^{d}\) 处的次梯度，并且 \(h(\cdot)\) 在 \(c\) 处的次梯度集合记为 \(\partial h(c)\)。对于两个矩阵 \(B_{1}, B_{2} \in \mathbb{R}^{d \times p}\)，迹内积记为 \(B_{1} \bullet B_{2} \coloneqq \operatorname{trace}(B_{1}^{T} B_{2})\)。最后，我们注意到该框架的名称灵感来源于 Farias (2007)。

\subsection{SPO 损失函数}\label{spo-loss-functions}

在此，我们介绍几种属于“先预测后优化”范式的损失函数，但它们同时也是“智能”的，因为在衡量预测误差时考虑了名义优化问题 \(P(\cdot)\)。我们将这些损失函数称为智能“先预测后优化”（SPO）损失函数。作为起点，让我们考虑一个真实的 SPO 损失函数，它精确地衡量了由于不精确的成本向量预测导致次优决策而产生的额外成本。遵循 PO 范式，给定一个成本向量预测 \(\hat{c}\)，基于求解 \(P(\hat{c})\) 来实施决策 \(w^{*}(\hat{c})\)。在决策 \(w^{*}(\hat{c})\) 实施后，所产生的成本是针对实际实现的成本向量 \(c\) 的。由于 \(w^{*}(\hat{c})\) 相对于 \(c\) 可能是次优的，因此产生的额外成本为 \(c^T w^{*}(\hat{c}) - z^{*}(c)\)，我们称之为 SPO 损失。

在图 1 中，我们展示了具有相同预测误差的两个 \(c\) 的预测值如何导致不同的决策和不同的 SPO 损失。我们考虑一个二维多面体和一个椭圆作为可行域 \(S\)。我们绘制了（负的）真实成本向量 \(c\)，以及两个与 \(c\) 等距的候选预测 \(\hat{c}_A\) 和 \(\hat{c}_B\)，因此它们具有等效的 LS 损失。可以看到，\(\hat{c}_A\) 的最优决策与 \(c\) 的最优决策一致，因为 \(w^*(\hat{c}_A) = w^*(c)\)，因此 SPO 损失为零。相反，我们看到 \(w^*(\hat{c}_B) \neq w^*(c)\)，因此导致正的 SPO 损失。在多面体示例中，任何负值不在灰色区域的预测成本向量都将导致正的 SPO 损失，而在椭圆示例中，任何不与 \(c\) 完全平行的预测成本向量都会导致正的 SPO 损失。定义 1 形式化了当实际成本向量为 \(c\) 时，进行预测 \(\hat{c}\) 所关联的真实 SPO 损失，给定一个特定的用于 \(P(\cdot)\) 的预言机 \(w^*(\cdot)\)。

**定义 1 (SPO 损失)**。 给定一个成本向量预测 \(\hat{c}\) 和一个已实现的成本向量 \(c\)，关于优化预言机 \(w^*(\cdot)\) 的真实 SPO 损失 \(\ell_{\mathrm{SPO}}^{w*}(\hat{c}, c)\) 定义为 \(\ell_{\mathrm{SPO}}^{w*}(\hat{c}, c) := c^T w^*(\hat{c}) - z^*(c)\)。

注意，定义 1 存在一个不足之处，即依赖于用于求解 (2) 的特定预言机 \(w^{*}(\cdot)\)。从实践角度讲，这个不足不是主要问题，因为我们通常应该期望 \(w^{*}(\hat{c})\) 是唯一的最优解——也就是说，我们应该期望 \(W^{*}(\hat{c})\) 是单点集。注意，如果损失函数可以使用 \(W^{*}(\hat{c})\) 中的任何解，那么损失函数本质上变为 \(\min_{w \in W^{*}(\hat{c})} c^{T} w - z^{*}(c)\)。这样，预测模型就会被激励总是做出退化的预测 \(\hat{c} = 0\)，因为 \(W^{*}(0) = S\)。这将意味着 SPO 损失为零。

无论如何，如果希望解决定义 1 中对特定预言机 \(w^{*}(\cdot)\) 的依赖，那么最自然的方式是假设实施的决策相对于 \(c\) 具有最坏情况行为来“打破平局”。定义 2 是一个替代的 SPO 损失函数，它不依赖于优化预言机 \(w^{*}(\cdot)\) 的特定选择。

**定义 2 (无歧义 SPO 损失)**。 给定一个成本向量预测 \(\hat{c}\) 和一个已实现的成本向量 \(c\)，（无歧义的）真实 SPO 损失 \(\ell_{\mathrm{SPO}}(\hat{c}, c)\) 定义为 \(\ell_{\mathrm{SPO}}(\hat{c}, c) := \max_{w \in W^{*}(\hat{c})} \{c^T w\} - z^{*}(c)\)。

注意，定义 2 提出了一个真实 SPO 损失的版本，它上界定于定义 1 的版本——也就是说，对于所有 \(\hat{c}, c \in \mathbb{R}^d\)，有 \(\ell_{\mathrm{SPO}}^{w*}(\hat{c}, c) \leq \ell_{\mathrm{SPO}}(\hat{c}, c)\)。如前所述，定义 1 和定义 2 之间的区别仅在退化情况下相关。在本文的结果和讨论中，我们使用定义 2 给出的无歧义真实 SPO 损失。通过回顾定义 2 上界定于定义 1 并且两个损失函数几乎总是相等（除了 \(W^{*}(\hat{c})\) 有多个最优解的退化情况），通常可以推断出定义 1 的真实 SPO 损失版本的相关结果。

注意 \(\ell_{\mathrm{SPO}}(\hat{c}, c)\) 不受 \(\hat{c}\) 缩放的影响；换句话说，对于所有 \(\alpha > 0\)，有 \(\ell_{\mathrm{SPO}}(\alpha \hat{c}, c) = \ell_{\mathrm{SPO}}(\hat{c}, c)\)。这个性质是直观的，因为与预测 \(\hat{c}\) 相关的真实损失应仅取决于 \(P(\cdot)\) 的最优解，而该解不依赖于 \(\hat{c}\) 的缩放。此外，这个性质也是二分类问题中 0-1 损失函数所共有的。即，标签可以取集合 \(\{-1, +1\}\) 中的值，预测模型预测 \(\mathbb{R}\) 中的值。如果预测值的符号与真实值相同，则损失为零，否则损失为一。也就是说，给定一个预测值 \(\hat{c} \in \mathbb{R}\) 和一个标签 \(c \in \{-1, +1\}\)，0-1 损失函数定义为 \(\ell_{0-1}(\hat{c}, c) := \mathbf{1}(\operatorname{sgn}(\hat{c}) = c)\)，其中 \(\operatorname{sgn}(\cdot)\) 是符号函数，\(\mathbf{1}(\cdot)\) 是指示函数，如果输入为真则等于 1，否则为 0。因此，0-1 损失函数也独立于预测的尺度。这种相似性并非巧合；实际上，命题 1 说明了二分类是 SPO 框架的一个特例。所有证明可以在在线附录 B 中找到。

**命题 1 (SPO 损失推广了 0-1 损失)**。 当 \(S = [-1 / 2, + 1 / 2]\) 且 \(c\in \{-1, + 1\}\) 时，有 \(\ell_{\mathrm{SPO}}(\hat{c},c) = \mathbf{1}(\operatorname{sgn}(\hat{c}) = c)\)，即 SPO 损失函数与二分类相关的 0-1 损失函数完全匹配。

现在，给定训练数据，我们感兴趣的是确定一个具有最小真实 SPO 损失的成本向量预测模型。因此，给定先前定义的真实 SPO 损失 \(\ell_{\mathrm{SPO}}(\cdot ,\cdot)\)，预测模型将通过遵循 (3) 中的经验风险最小化原则来确定，这导致以下优化问题：

\[
\min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {\mathrm {S P O}} \left(f \left(x _ {i}\right), c _ {i}\right). \tag {4}
\]

不幸的是，上述优化问题在理论和实践上都难以求解。实际上，对于固定的 \(c\)，\(\ell_{\mathrm{SPO}}(\cdot, c)\) 在 \(\hat{c}\) 上甚至可能不连续，因为 \(w^{*}(\hat{c})\)（以及整个集合 \(W^{*}(\hat{c})\)）在 \(\hat{c}\) 上可能不连续。此外，由于命题 1 表明我们的框架包含了二分类，求解 (4) 至少与优化 0-1 损失函数一样困难，这在许多情况下可能是 NP 难的（Ben-David 等人 2003）。因此，我们有动机开发方法来产生 (4) 的“合理”近似解，这些解 (i) 优于标准的 PO 方法，并且 (ii) 适用于训练样本数量 \(n\) 和/或假设类 \(\mathcal{H}\) 的维度可能非常大的大规模问题。

\subsubsection{说明性示例}\label{an-illustrative-example}

为了建立直观理解，我们现在通过一个说明性示例来比较 SPO 损失与经典的最小二乘损失函数。考虑一个非常简单的具有两个节点 \(s\) 和 \(t\) 的最短路径问题。有两条从 \(s\) 到 \(t\) 的边，边 1 和边 2。因此，在此设置中成本向量 \(c\) 是二维的，目标是简单地选择成本较低的边。在决策时我们不会直接观察 \(c\)，而是观察与向量 \(c\) 相关的一维特征 \(x\)。我们的数据由 \((x_{i}, c_{i})\) 对组成，并且 \(c_{i}\) 是 \(x_{i}\) 的非线性函数生成的。

在图 2(a) 中，LS 损失函数的残差由虚线标出。残差是预测值与真实值之间的距离。在图 2(b) 中，SPO 损失函数的残差由黑色虚线标出。当预测值的顺序正确时，残差为零。否则，残差是真实值之间的距离。

决策者的目标是通过使用简单的线性回归模型从特征预测每条边的成本。两条线（对应于每条边）的交点将在“先预测后优化”框架中指示决策边界。决策者将尝试使用 SPO 和 LS 损失函数进行线性回归。在图 2 中，我们通过可视化一个特定数据集和用于预测边 1 和边 2 成本的线性模型的残差来说明 LS 和 SPO 之间的差异。在 LS 回归中，最小化残差平方和，如图 2(a) 中的绿色和红色虚线所示。当使用 SPO 损失时，我们考虑“决策残差”，这仅在预测导致选择错误边时发生。在这些情况下，SPO 成本是边 1 和边 2 的两个真实成本之间的幅度差，如图 2(b) 中的黑色虚线所示。


**图 2. 预测残差与决策残差的差异**

注释. (a) 预测残差. (b) 决策残差. Pred., 预测.



注释. (a) 预测残差. (b) 决策残差. Pred., 预测.

在图 3 中，我们考虑另一个数据集，但这次绘制了最优的 LS 和 SPO 线性回归模型。在图 3 的左上方面板中，我们绘制了数据集和最优决策边界。在右上面板中，我们绘制了对数据的最佳 LS 拟合，在下面两个面板中，我们绘制了 SPO 线性回归的两个不同的最优解。（实际上，SPO 拟合模型对于 \(\mathrm{SPO + }\) 损失也是最优的，我们将在第 3.2 节推导。）垂直虚线对应于在真实模型和预测模型下的决策边界。注意，图 3 中的 SPO 损失为零，因为不存在图 2 中描述的那些决策错误。

从图 3 可以看出，LS 线非常接近地逼近非线性数据，尽管 LS 的决策边界与最优决策边界相距甚远。对于介于黑色和红色虚线之间的任何 \(x\) 值，决策者将选择错误的边。相比之下，SPO 线根本不需要很好地逼近数据，但它们的决策边界近乎最优。实际上，SPO 线的训练误差为零，尽管根本没有拟合数据。关键的直觉在于，任何时候选择了错误的边都会产生 SPO 损失，并且在这个例子中，可以构造在正确决策边界处相交的线，从而永远不会选择错误的边，


导致零 SPO 损失。注意，唯一重要的考虑因素是线在何处相交，因此 SPO 线性回归不一定最小化预测误差。当然，可以使用 SPO 和 LS 损失的凸组合来克服生成的不寻常的线。实际上，对于 SPO 损失的 ERM 问题存在无限多个最优解，所有这些解只需要线的交点在 1.2 和 1.3 的 \(x\) 值之间即可。

\subsubsection{SPO+ 损失函数}\label{the-spo-loss-function}

在本节中，我们专注于推导一个可处理的代理损失函数，它能合理地近似 \(\ell_{\mathrm{SPO}}(\cdot ,\cdot)\)。我们的代理函数 \(\ell_{\mathrm{SPO + }}(\cdot ,\cdot)\)，我们称之为 \(\mathrm{SPO + }\) 损失函数，可以通过几个步骤推导出来，我们将在下面仔细论证这些步骤。理想情况下，当找到使用 \(\mathrm{SPO + }\) 损失最小化经验风险的预测模型时，该预测模型也将近似最小化使用 SPO 损失的经验风险 (4)。

为了开始推导 SPO+ 损失，我们首先观察到，对于任何 \(\alpha \in \mathbb{R}\)，SPO 损失可以写为

\[
\ell_ {\mathrm {S P O}} (\hat {c}, c) = \max  _ {w \in W ^ {*} (\hat {c})} \left\{c ^ {T} w - \alpha \hat {c} ^ {T} w \right\} + \alpha z ^ {*} (\hat {c}) - z ^ {*} (c), \tag {5}
\]

因为对于所有 \(w\in W^{*}(\hat{c})\)，有 \(z^{*}(\hat{c}) = \hat{c}^{T}w\)。显然，在 (5) 中将约束 \(w\in W^{*}(\hat{c})\) 替换为 \(w\in S\) 会导致一个上界。因为这对所有 \(\alpha\) 的值都成立，所以

\[
\ell_ {\mathrm {S P O}} (\hat {c}, c) \leq \inf  _ {\alpha} \left\{\max  _ {w \in S} \left\{c ^ {T} w - \alpha \hat {c} ^ {T} w \right\} + \alpha z ^ {*} (\hat {c}) \right\} - z ^ {*} (c). \tag {6}
\]

实际上，可以使用对偶理论证明不等式 (6) 实际上是一个等式，而且，\(\alpha\) 的最优值趋向于 \(\infty\)。直观上，可以看到当 \(\alpha\) 变大时，内部最大化目标中的项 \(c^T w\) 变得可忽略不计，解趋向于 \(w^{*}(\alpha \hat{c}) = w^{*}(\hat{c})\)。因此，当 \(\alpha\) 趋向于 \(\infty\) 时，对 \(S\) 的内部最大化可以替换为对 \(W^{*}(\hat{c})\) 的最大化，这就恢复了 (5)。我们在下面的命题 2 中形式化了这种等价性。

**命题 2 (SPO 损失的对偶表示)**。 对于任何成本向量预测 \(\hat{c} \in \mathbb{R}^d\) 和已实现的成本向量 \(c \in \mathbb{R}^d\)，函数 \(\alpha \mapsto \max_{w \in S} \{c^T w - \alpha \hat{c}^T w\} + \alpha z^*(\hat{c})\) 在 \(\mathbb{R}\) 上是单调递减的，并且真实 SPO 损失函数可以表示为

\[
\ell_ {\mathrm {S P O}} (\hat {c}, c) = \lim  _ {\alpha \rightarrow \infty} \left\{\max  _ {w \in S} \left\{c ^ {T} w - \alpha \hat {c} ^ {T} w \right\} + \alpha z ^ {*} (\hat {c}) \right\} - z ^ {*} (c). \tag {7}
\]

使用命题 2，我们现在将重新审视 SPO ERM 问题 (4)，它可以写为

\[
\begin{array}{l} \min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \lim  _ {\alpha_ {i} \rightarrow \infty} \left\{\max  _ {w \in S} \left\{c _ {i} ^ {T} w - \alpha_ {i} f (x _ {i}) ^ {T} w \right\}\right. \\ \left. + \alpha_ {i} z ^ {*} (f (x _ {i})) \right\} - z ^ {*} (c _ {i}) \\ = \min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \lim  _ {\alpha_ {i} \rightarrow \infty} \left\{\max  _ {w \in S} \left\{c _ {i} ^ {T} w - \alpha_ {i} f (x _ {i}) ^ {T} w \right\}\right. \\ \left. + \alpha_ {i} f (x _ {i}) ^ {T} w ^ {*} \left(\alpha_ {i} f (x _ {i})\right) \right\} - z ^ {*} (c _ {i}) \\ = \min  _ {f \in \mathcal {H}} \frac {1}{n} \lim  _ {\alpha \rightarrow \infty} \left\{\sum_ {i = 1} ^ {n} \max  _ {w \in S} \left\{c _ {i} ^ {T} w - \alpha f (x _ {i}) ^ {T} w \right\}\right. \\ \left. + \alpha f (x _ {i}) ^ {T} w ^ {*} (\alpha f (x _ {i})) - z ^ {*} (c _ {i}) \right\} \\ \leq \min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \max  _ {w \in S} \left\{c _ {i} ^ {T} w - 2 f (x _ {i}) ^ {T} w \right\} \\ + 2 f \left(x _ {i}\right) ^ {T} w ^ {*} \left(2 f \left(x _ {i}\right)\right) - z ^ {*} \left(c _ {i}\right), (8) \\ \leq \min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \max  _ {w \in S} \left\{c _ {i} ^ {T} w - 2 f (x _ {i}) ^ {T} w \right\} + \\ 2 f \left(x _ {i}\right) ^ {T} w ^ {*} \left(c _ {i}\right) - z ^ {*} \left(c _ {i}\right). (9) \\ \end{array}
\]

第一个等式源于对于任何 \(\alpha_{i} > 0\)，有 \(z^{*}(\alpha_{i}f(x_{i})) = \alpha_{i}z^{*}(f(x_{i}))\)。第二个等式源于观察到所有 \(\alpha_{i}\) 变量都趋向于相同的值，因此我们可以用一个变量替换它们，我们称之为 \(\alpha\)。第一个不等式源于命题 2，特别是，在 (6) 中设置 \(\alpha = 2\) 会得到 SPO 损失的一个上界（我们将在下面重新讨论这个特定选择）。最后，第二个不等式源于 \(w^{*}(c_{i})\) 是 \(P(2f(x_{i}))\) 的一个可行解。

(9) 中的求和项表达式正是我们所说的 SPO+ 损失函数，我们在定义 3 中正式说明。

**定义 3 (SPO+ 损失)**。 给定一个成本向量预测 \(\hat{c}\) 和一个已实现的成本向量 \(c\)，SPO+ 损失定义为 \(\ell_{\mathrm{SPO + }}(\hat{c},c)\coloneqq \max_{w\in S}\{c^T w - 2\hat{c}^T w\} +2\hat{c}^T w^* (c) - z^* (c)\)。

回顾 \(\xi_S(\cdot)\) 是 \(S\) 的支撑函数——即 \(\xi_{S}(c)\coloneqq \max_{w\in S}\{c^{T}w\}\)。使用这个符号，\(\mathrm{SPO + }\) 损失可以等价地表示为 \(\ell_{\mathrm{SPO + }}\) \((\hat{c},c) = \xi_S(c - 2\hat{c}) + 2\hat{c}^T w^* (c) - z^* (c)\)。

在继续之前，我们将提供理由说明为什么用于推导 \(\mathrm{SPO}+\) 的不等式 (8) 和 (9) 确实是合理的近似。虽然不等式 (8) 可以在没有前面中间步骤的情况下推导出来，但我们现在声称这个不等式对于许多假设类实际上是一个等式。即，对于任何假设类 \(\mathcal{H}\)，其中 \(f\in \mathcal{H}\) 意味着对于所有 \(\alpha \geq 0\) 有 \(\alpha f\in \mathcal{H}\)，那么该不等式是紧的，因为最小化 \(\alpha f\) 等价于最小化 \(2f\)。例如，线性模型的假设类满足这个性质，因为线性模型的所有标量倍数也是线性的。注意，\(\alpha\) 可以被吸收到假设类中是可能的，因为每个求和项中的 \(\alpha_{i}\) 项可以被一个单一的 \(\alpha\) 替换，因为它们都趋向于无穷大。我们特别选择 \(\alpha = 2\)（而不是任何其他正标量），是因为（在某些条件下）SPO+ 损失的贝叶斯风险最小化器恰好是 \(\mathbb{E}[c|x]\)，而不是 \(\mathbb{E}[c|x]\) 的倍数。这个概念将在第 4 节中形式化。

推导我们的凸代理 SPO+ 损失函数的最后一步 (9) 涉及用一阶展开来近似凹（非凸）函数 \(z^{*}(\cdot)\)。即，我们应用不等式 \(z^{*}(2f(x_{i})) = 2z^{*}(f(x_{i}))\leq 2f(x_{i})^{T}w^{*}(c_{i})\)，这可以看作是基于在 \(c_{i}\) 处计算的超梯度对 \(z^{*}(f(x_{i}))\) 的一阶近似（即，有 \(w^{*}(c_{i})\in \partial z^{*}(c_{i})\)）。注意，如果 \(f(x_{i}) = c_{i}\)，那么 \(\ell_{\mathrm{SPO}}(f(x_i),c_i) = \ell_{\mathrm{SPO + }}(f(x_i),c_i) = 0\)，这意味着在最小化 SPO+ 时，直观上，我们试图让 \(f(x_{i})\) 接近 \(c_{i}\)。因此，可以预期 \(w^{*}(c_{i})\) 是 \(P(2f(x_i))\) 的一个近似最优解，因此，不等式 (9) 将是一个合理的近似。实际上，第 4 节提供了一个在某些假设下的一致性性质，这表明如果预测模型在足够大的数据集上训练，预测 \(f(x_{i})\) 确实合理地接近 \(c_{i}\) 的期望值。

接下来，我们陈述以下命题，它形式化地表明 SPO+ 损失是 SPO 损失的一个上界，并且在 \(\hat{c}\) 上是凸的。注意，尽管 SPO+ 损失在 \(\hat{c}\) 上是凸的，但通常不可微，因为 \(\xi_{S}(\cdot)\) 通常不可微。然而，命题 3 也表明 \(2(w^{*}(c) - w^{*}(2\hat{c} - c))\) 是 SPO+ 损失的一个次梯度，这将在第 5 节开发计算方法时被利用。

**命题 3 (SPO+ 损失性质)**。 给定一个固定的已实现成本向量 \(c\)，有以下结论：

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.}
	\item
	对于所有 \(\hat{c}\in \mathbb{R}^d\)，有 \(\ell_{\mathrm{SPO}}(\hat{c},c)\leq \ell_{\mathrm{SPO + }}(\hat{c},c)\)\\
	\item
	\(\ell_{\mathrm{SPO + }}(\hat{c},c)\) 是成本向量预测 \(\hat{c}\) 的凸函数，且\\
	\item
	对于任何给定的 \(\hat{c}\)，\(2(w^{*}(c) - w^{*}(2\hat{c} - c))\) 是 \(\ell_{\mathrm{SPO + }}(\cdot)\) 在 \(\hat{c}\) 处的一个次梯度——即 \(2(w^{*}(c) - w^{*}(2\hat{c} - c)) \in \partial \ell_{\mathrm{SPO + }}(\hat{c}, c)\)。
\end{enumerate}

\(\mathrm{SPO + }\) 损失函数的凸性也是合页损失函数所共有的，合页损失是 0-1 损失函数的一个凸上界。回顾一下，给定预测 \(\hat{c}\)，如果真实标签是 1，则合页损失为 \(\max \{0,1 - \hat{c}\}\)，如果真实标签是 \(-1\)，则为 \(\max \{0,1 + \hat{c}\}\)。更简洁地说，合页损失可以写为 \(\max \{0,1 - c\hat{c}\}\)，其中 \(c\in \{-1, + 1\}\) 是真实标签。

合页损失是支持向量机方法的核心，在那里它被用作最小化 0-1 损失的凸代理。回顾一下，在这种二分类设置中，SPO 损失精确地捕获了 0-1 损失，如命题 1 所形式化。在相同的设置中，结果证明 \(\mathrm{SPO + }\) 损失等于在 \(2\hat{c}\)（即两倍的预测值）处评估的合页损失——这将在下面的命题 4 中形式化。这种轻微的差异是由于我们在上述推导 \(\mathrm{SPO + }\) 损失时选择了 \(\alpha = 2\)；选择 \(\alpha = 1\) 将得到精确的合页损失。

**命题 4 (SPO+ 损失推广了合页损失)**。 在与命题 1 相同的条件下——即当 \(S = [-1/2, +1/2]\) 且 \(c \in \{-1, +1\}\) 时——有 \(\ell_{\mathrm{SPO+}}(\hat{c}, c) = \max \{0, 1 - 2c\hat{c}\}\)——即 SPO+ 损失函数等价于与二分类相关的合页损失函数。

**备注 1 (与结构化预测的联系)**。 值得指出的是，先前描述的 SPO+ 损失的构造与结构化支持向量机中结构化合页损失的构造（Taskar 等人 2004, 2005; Tsochantaridis 等人 2005; Nowozin 和 Lampert 2011）有一些相似之处。此外，我们的问题设置通过利用名义优化问题的目标成本来自然定义 SPO 损失函数，从而扩展了结构化预测的设置。也就是说，如果我们定义 \(w_{i}^{*} \coloneqq w^{*}(c_{i})\)，那么修改后的数据集 \((x_{1},w_{1}^{*}),(x_{2},w_{2}^{*}),\ldots ,(x_{n},w_{n}^{*})\) 可以被视为结构化预测问题的训练数据。然而，这种约简丢弃了关于成本向量 \(c_{i}\) 的宝贵信息，而 SPO+ 损失函数自然地利用这些信息并上界于 SPO 损失。因此，我们的框架（以及代理 SPO+ 损失函数）可以被视为对 SSVM 问题（以及结构化合页损失）在存在自然成本结构设置下的一种改进。注意，SPO+ 损失和结构化合页损失都将二分类的常规合页损失作为特例。合页损失相对于 0-1 损失满足一个关键的一致性性质（Steinwart 2002），这证明了其在实践中的使用。在第 4 节中，我们展示了在一些温和条件下，SPO+ 损失相对于 SPO 损失的类似一致性结果。另一方面，结构化合页损失通常是不一致的（参见，例如，Zhang 2004 中围绕方程 (11) 的讨论），尽管在多类分类和结构化预测中已经有一些关于刻画一致性损失函数性质的结果（Zhang 2004, Tewari 和 Bartlett 2007, Osokin 等人 2017）。

**备注 2 (当 \(P(\cdot)\) 是组合或混合整数问题时)**。 如前所述，假设 \(S\) 是凸和闭的不会失去一般性，因为可以简单地在 (2) 中用可能非凸或非闭集的闭凸包替换它，而不改变最优值 \(z^{*}(c)\)。更具体地说，假设 \(\tilde{S} \subseteq \mathbb{R}^d\) 是一个有界但可能非凸或非闭的集合，并且 \(S\) 是 \(\tilde{S}\) 的闭凸包。进一步假设预言机 \(w^{*}(\cdot)\) 返回 \(\tilde{S}\) 中的一个最优解——即对于所有 \(c \in \mathbb{R}^d\)，有 \(w^{*}(c) \in \arg \min_{w \in \tilde{S}} c^{T} w \subseteq \arg \min_{w \in S} c^{T} w\)。例如，如果 \(\tilde{S}\) 表示一个组合或混合整数优化问题的可行域，那么预言机将对应于该问题的一个实际有效的算法。然后，利用在 \(\tilde{S}\) 上的线性优化等价于在 \(S\) 上的线性优化这一事实，很容易看出，关于 \(\tilde{S}\) 定义的 SPO 和 SPO+ 损失函数恰好等于关于 \(S\) 定义的相应损失函数。最后，使用命题 3，可以利用预言机 \(w^{*}(c) \in \arg \min_{w \in \tilde{S}} c^{T} w\) 来计算 SPO+ 损失函数的次梯度，这可以用于计算方法中，如第 5 节所述。

将如 (4) 中的 ERM 原理应用于 SPO+ 损失，得到以下用于选择预测模型的优化问题：

\[
\min  _ {f \in \mathcal {H}} \frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {\mathrm {S P O} +} (f (x _ {i}), c _ {i}). \tag {10}
\]

本文大部分剩余部分描述了关于问题 (10) 的结果。在第 4 节中，我们展示了前面提到的 Fisher 一致性结果；在第 5 节中，我们描述了几种求解问题 (10) 的计算方法；在第 6 节中，我们证明了 (10) 通常比标准的 PO 方法提供更优越的实际性能。接下来，我们为使用 \(\mathrm{SPO + }\) 损失提供一个理论动机的证明。

\subsection{SPO+ 损失函数的一致性}\label{consistency-of-the-spo-loss-function}

在本节中，我们证明一个基本的一致性性质，称为 Fisher 一致性，以描述最小化 SPO+ 损失在何种情况下等价于最小化 SPO 损失。替代损失函数的 Fisher 一致性意味着，在完全了解数据分布且不对假设类加以限制的情况下，最小化替代损失函数的函数同时也最小化真实损失（Lin 2004, Zou et al.~2008）。人们也可以说替代损失函数与真实损失函数是校准的（Bartlett et al.~2006）。我们的结果类似于铰链损失和逻辑损失函数相对于 0-1 损失的著名一致性结果——在完全信息下最小化铰链损失和逻辑损失也最小化了 0-1 损失——并为其在实践中的成功提供了理论动机。

更正式地，我们令 \(\mathcal{D}\) 表示 \((x,c)\) 的分布，即 \((x,c)\sim \mathcal{D}\)，并考虑真实 SPO 风险（贝叶斯风险）最小化问题的总体形式：

\[
\min  _ {f} \mathbb {E} _ {(x, c) \sim \mathcal {D}} \left[ \ell_ {\text {SPO}} (f (x), c) \right], \tag {11}
\]

以及 SPO+ 风险最小化问题的总体形式：

\[
\min  _ {f} \mathbb {E} _ {(x, c) \sim \mathcal {D}} \left[ \ell_ {\mathrm {SPO} +} (f (x), c) \right]. \tag {12}
\]

请注意，这里我们对 \(f(\cdot)\) 没有施加任何限制，这意味着 \(\mathcal{H}\) 包含任何将特征映射到成本向量的可测函数。

**定义 4 (Fisher 一致性)**。如果 \(\arg \min_{f} \mathbb{E}_{(x,c) \sim \mathcal{D}}[\ell(f(x), c)]\)（即 \(\ell\) 的贝叶斯风险的最小化点集合）也最小化 (11)，则称损失函数 \(\ell(\cdot, \cdot)\) 关于 SPO 损失是 Fisher 一致的。

为了获得一些直观理解，令 \(f_{\mathrm{SPO}}^*\) 和 \(f_{\mathrm{SPO+}}^*\) 分别表示 (11) 和 (12) 的任意最优解。从 (1) 式可以看出，\(f_{\mathrm{SPO}}^*(x)\) 的一个理想值就是 \(\mathbb{E}[c|x]\)。事实上，只要 \(P(\mathbb{E}[c|x])\) 的最优解以概率一（在 \(x \in \mathcal{X}\) 的分布上）是唯一的——即几乎必然——那么确实，\(\mathbb{E}[c|x]\) 是 (11) 的一个最小化点（见命题 5）。此外，任何几乎必然等于 \(\mathbb{E}[c|x]\) 的函数也是 (11) 的一个最小化点。在定理 1 中，我们证明在假设 1 下，任何 SPO+ 总体风险 (12) 的最小化点必须几乎必然满足 \(f_{\mathrm{SPO+}}^*(x) = \mathbb{E}[c|x]\)，因此也最小化了 SPO 风险 (11)。总之，在假设 1 下，SPO+ 损失关于 SPO 损失是 Fisher 一致的。

**假设 1**。这些假设意味着 \(SPO +\) 损失函数的 Fisher 一致性：

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.}
	\item
	几乎必然地，\(W^{*}(\mathbb{E}[c|x])\) 是一个单点集——即 \(\mathbb{P}_x(|W^* (\mathbb{E}[c|x])| = 1) = 1\)。\\
	\item
	对于所有 \(x \in \mathcal{X}\)，给定 \(x\) 的条件下 \(c\) 的分布关于其均值 \(\mathbb{E}[c|x]\) 是中心对称的。\\
	\item
	对于所有 \(x \in \mathcal{X}\)，给定 \(x\) 的条件下 \(c\) 的分布在整个 \(\mathbb{R}^d\) 上是连续的。\\
	\item
	可行域 \(S\) 的内部是非空的。
\end{enumerate}

**定理 1 (\(SPO+\) 的 Fisher 一致性)**。假设假设 1 成立。那么，任何 \(SPO+\) 风险 (12) 的最小化点几乎必然（在 \(x \in \mathcal{X}\) 的分布上）等于 \(\mathbb{E}[c|x]\)，并且也是 \(SPO\) 风险 (11) 的一个最小化点。因此，\(SPO+\) 损失函数关于 \(SPO\) 损失是 Fisher 一致的。

证明定理 1 的关键结果在第 4.1 节中给出，最终证明在在线附录中提供。我们指出，假设 1(1) 仅用于证明 \(\mathbb{E}[c|x]\) 是 SPO 风险的一个最小化点。

这个假设是相当温和的，因为具有多个最优解的点集通常测度为零。实际上，如果使用 SPO 损失的定义 1（该定义使用给定的优化预言机），则可以移除假设 1(1)。假设 1(2) 确保 \(\mathbb{E}[c|x]\) 是 \(\mathrm{SPO + }\) 风险的一个最小化点。注意，一个随机向量 \(d\) 关于其均值是中心对称的，如果 \(d - \mathbb{E}[d]\) 与 \(\mathbb{E}[d] - d\) 同分布，或者等价地，\(d\) 与 \(2\mathbb{E}[d] - d\) 同分布。例如，当假设数据形式为 \(f(x) + \epsilon\)，其中 \(\epsilon\) 是均值为零、协方差矩阵半正定的高斯分布时，满足此对称条件。最后，假设 1(3) 和假设 1(4) 都是标准假设，用于证明 \(\mathbb{E}[c|x]\) 是 \(\mathrm{SPO + }\) 风险的唯一最小化点，除了一个概率测度为零的集合可能例外。注意，假设 1(2) 和假设 1(3) 可以放宽为关于 \(x\in \mathcal{X}\) 的概率测度几乎必然成立，但为了表述简便，我们对所有 \(x\in \mathcal{X}\) 陈述它们。在第 4.1 节中，我们讨论（在线附录中提供的）例子，这些例子展示了如果其中一个假设不成立，我们的结果可能不成立的情况。

如前所述，任何最小二乘风险的最小化点也几乎必然等于 \(\mathbb{E}[c|x]\)，因此，最小二乘损失关于 SPO 损失也是 Fisher 一致的。因此，先验地，不能声称 LS 或 \(\mathrm{SPO + }\) 比对方更好。确实，我们从 SPO 损失直接推导出了一个自然的替代损失函数 \(\mathrm{SPO + }\)，它保持了事实上的标准 LS 损失函数的基本一致性性质。事实上，容易看出，在假设 1 下，LS 和 \(\mathrm{SPO + }\) 损失函数的任何凸组合都是 Fisher 一致的。由于这个一致性性质是在完全分布信息且没有模型设定错误（对假设类无限制）的情况下适用的，我们在第 6 节展示，由于其能够将预测定制到优化任务，\(\mathrm{SPO + }\) 在几个实验设置中确实优于 LS。

\subsubsection{证明 Fisher 一致性的关键结果}\label{key-results-to-prove-fisher-consistency}

在本节中，我们考虑一个非参数设置，其中不失一般性地忽略了对特征 \(x\) 的依赖。要理解这一点，首先观察到 SPO 风险满足 \(\mathbb{E}_{(x,c)\sim \mathcal{D}}[\ell_{\mathrm{SPO}}(f(x),c)] = \mathbb{E}_x[\mathbb{E}_c[\ell_{\mathrm{SPO}}(f(x),c)\mid x]]\)，SPO+ 风险也类似。因为对 \(f(\cdot)\) 没有约束（假设类包含所有预测模型），所以求解问题 (11) 和 (12) 等价于对所有 \(x\in \mathcal{X}\) 单独优化每个函数值 \(f(x)\)。因此，在本节剩余部分，除非另有说明，我们忽略对 \(x\) 的依赖。因此，我们现在假设分布 \(\mathcal{D}\) 仅

是关于 \(c\) 的，并且 SPO 和 \(\mathrm{SPO + }\) 风险分别定义为 \(R_{\mathrm{SPO}}(\hat{c})\coloneqq \mathbb{E}_c[\ell_{\mathrm{SPO}}(\hat{c},c)]\) 和 \(R_{\mathrm{SPO + }}(\hat{c})\coloneqq \mathbb{E}_c[\ell_{\mathrm{SPO + }}(\hat{c},c)]\)。为方便起见，我们定义 \(\bar{c}\coloneqq \mathbb{E}_c[c]\)（注意我们隐含地假设 \(\bar{c}\) 是有限的）。

接下来，我们在此设置下完全刻画了真实 SPO 风险问题 (11) 的最小化点。命题 5 表明，对于 \(R_{\mathrm{SPO}}(\cdot)\) 的任何最小化点 \(c^*\)，其关于名义问题的所有对应解 \(W^{*}(c^{*})\) 也是 \(P(\bar{c})\) 的最优解。换句话说，最小化真实 SPO 风险也优化了名义问题中的期望成本（因为目标函数是线性的）。命题 5 还证明了反之亦然——即，任何具有唯一最优解且同时优化了期望成本的成本向量预测也是真实 SPO 风险的一个最小化点。

**命题 5 (SPO 最小化点)**。如果成本向量 \(c^*\) 是 \(R_{\mathrm{SPO}}(\cdot)\) 的一个最小化点，那么 \(W^*(c^*) \subseteq W^*(\overline{c})\)。反之，如果 \(c^*\) 是一个成本向量，使得 \(W^*(c^*)\) 是单点集且 \(W^*(c^*) \subseteq W^*(\overline{c})\)，那么 \(c^*\) 是 \(R_{\mathrm{SPO}}(\cdot)\) 的一个最小化点。

在线附录 A 中的例 2 表明，为了确保 \(c^*\) 是 \(R_{\mathrm{SPO}}(\cdot)\) 的一个最小化点，仅仅允许 \(c^*\) 是任何满足 \(W^*(c^*) \subseteq W^*(\bar{c})\) 的成本向量是不够的。事实上，即使 \(c^*\) 是 \(\bar{c}\) 也可能不够。这是由于 SPO 损失函数的明确性，它在预测允许多个最优解的情况下选择了一个最坏情况的最优解。

接下来，我们给出命题 6，它展示了 \(\bar{c}\) 是 SPO+ 风险最小化点的充分条件，因此也是 SPO 风险的最小化点，从而意味着 Fisher 一致性。我们还给出了 \(\bar{c}\) 是 SPO+ 风险唯一最小化点的条件，这减轻了可能存在其他不满足 Fisher 一致性的 SPO+ 风险最小化点的担忧。

**命题 6 (SPO+ 最小化点)**。假设 \(c\) 的分布 \(\mathcal{D}\) 是连续的且关于其均值 \(\bar{c}\) 中心对称（即，\(c\) 与 \(2\bar{c} - c\) 同分布）。

\begin{enumerate}
	\def\labelenumi{\alph{enumi}.}
	\item
	那么，\(\bar{c}\) 最小化 \(R_{\mathrm{SPO + }}(\cdot)\)。\\
	\item
	此外，假设 \(S\) 的内部是非空的。那么，\(\bar{c}\) 是 \(R_{\mathrm{SPO}_+}(\cdot)\) 的唯一最小化点。
\end{enumerate}

命题 6 中的两个重要假设是 \(\mathcal{D}\) 关于其均值中心对称和连续，这两个假设各自单独并不足以确保一致性。在线附录 A 中的例 3 展示了一种情况，其中 \(c\) 在 \(\mathbb{R}^d\) 上是连续的且 \(\mathrm{SPO + }\) 的最小化点是唯一的，但它并不最小化 SPO 风险。在线附录 A 中的例 4 展示了一种情况，其中 \(c\) 的分布关于其均值对称，但存在一个 \(\mathrm{SPO + }\) 风险的最小化点不最小化 SPO 风险。在线附录 A 中的例 5 展示了一种情况，其中

如果 \(S\) 的内部是空的，则 \(\mathrm{SPO + }\) 的最小化点不是唯一的，而 \(c\) 是连续且关于其均值中心对称的。

